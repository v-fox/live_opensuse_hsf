#
# tuned configuration
#

[main]
summary=Less power-hungry version of latency-performance profile
include=latency-performance

[bootloader]
cmdline=clocksource=hpet nvme.io_queue_depth=16384 nvme.max_host_mem_size_mb=1024 nvme_core.admin_timeout=900 nvme_core.io_timeout=600 nvme_core.shutdown_timeout=30 nvme_core.streams=1

[cpu]
# this setting is possibly no longer applicable.
# 'schedutil' is considered universal future-proof solution but it's too undercooked yet.
# 'ondemand' is safe default for all cases.
governor=schedutil

[vm]
# https://www.kernel.org/doc/Documentation/vm/transhuge.txt
# use THP (transparent hugepages of 2M instead of default 4K on x86) to speed up RAM management by avoiding fragmentation & memory controller overload for price of more size overhead.
# 'always' forces them by default, use only with a lot of RAM to spare. Better used in conjunction with kernel boot option of 'transparent_hugepage=always' for early allocation.
# 'madvise' relies on software explicitly requesting them which is the safe option
#transparent_hugepages=always

[script]
# script for autoprobing sensors
script=script.sh

[sysfs]
# force full speed at the first core of the first CPU so
# non-demanding realtime programs, such as JACK, may be forced on it
#/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor=performance

# don't run KSM too often
/sys/kernel/mm/ksm/sleep_millisecs=250

# accumpanying options for THP
# tmpfs mounts also should have 'huge=' option with 'always', 'within_size' or 'advise', like in /etc/systemd/system/{dev-shm,tmp}.mount
/sys/kernel/mm/transparent_hugepage/shmem_enabled=advise
# 'defer+madvise' is the most preffered option but it has a small risk of delays breaking realtime, 'always' breaks realtime for sure and creates bad stutters.
# however, tuned seems to shit itself and refuses to use 'defer'.
/sys/kernel/mm/transparent_hugepage/defrag=defer+madvise
# separate periodic defrag call is also breaks realtime and creates bad stutters
/sys/kernel/mm/transparent_hugepage/khugepaged/defrag=0
# make it often and not long to avoid stutters
/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan=512
/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs=1000
/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs=10000
# for x86_64 arch
#/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_none=511
#/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_swap=511

# NVMe I/O scheduling optimizations for minimal random r/w latency
# accommodates kernel NVMe module options above
# https://www.kernel.org/doc/Documentation/block/
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/nr_requests=2048
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/read_ahead_kb=0
# device-specific, must be equal to max_hw_sectors_kb
#/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/max_sectors_kb=2048
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queueio_poll_delay=0
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/nomerges=1
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/iosched/front_merges=0
# must be lower for better latency
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/iosched/fifo_batch=4
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/iosched/read_expire=50
/sys/devices/pci*/*/*/nvme/nvme*/nvme*n*/queue/iosched/write_expire=1000

## attempt in controlling PWM fans
# we need to make sure that all this is done after module probing and autoloading
# it would be nice to set it to not spin lower than 2 times per second (120 RPM) BUT if that's not 0 then fan alarm will loose its shit
#/sys/class/hwmon/hwmon?/fan?_min=0
# SHUT UP
#/sys/class/hwmon/hwmon?/fan?_beep=0
#/sys/class/hwmon/hwmon?/fan?_alarm=0

# modern CPUs and their thermal pastes are designed to be no hotter than 70 degrees
# attempt to force "use fan at 32-70 degrees Celsius where 70 is critical, try to use ~80% at initialization and ~100% fan speed at 60 degrees" rule on motherboard
/sys/class/hwmon/hwmon?/temp?_min=32000
/sys/class/hwmon/hwmon?/temp?_max=67000
/sys/class/hwmon/hwmon?/temp?_crit_hyst=70000
/sys/class/hwmon/hwmon?/temp?_crit=73000
# 23437 is a default safe value (anything above hearing frequency of 22 KHz, about 25 KHz), lower ones (like 10190) may work better but they also produce noticeable noise
#/sys/class/hwmon/hwmon?/pwm?_freq=46875
# apparently, 0="full auto" (temperature-based ?), 1="open-loop" (manual, based on value 0-255 in pwm?) and 2="closed-loop" (based on target RPM or some other weird crap)
# on my system with broken it87 module setting '0' forces constant 100%, '1' forces constant hardcoded value (and wrong one at that) and '2' is getting stuck near last used value
# '2' may require fan IDs to be consistens with temperature sensor IDs which is unrealistic (for me they are reversed)
#/sys/class/hwmon/hwmon?/pwm?=145
# this may behave weirdly if applied to all fans at once, so do it only for the CPU one
#/sys/class/hwmon/hwmon?/pwm1_enable=2

# AMD GPU is locked to about 20% for some reason by default, try to force it to spin at 100% close to 65 degrees and set about 60% by default, so it would, at worst, be stuck on a useful speed
# 0 seem to be forcing constant 100% and 1 is fully manual
#/sys/class/drm/card?/device/hwmon/hwmon?/pwm?=128
/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_min=64
/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_max=192
# on '2' amdgpu get's stuck at 13XX and on 0 it's just 100% meaning that this whole thing is broken
#/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_enable=2
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_min=32000
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_max=67000
# GPUs are usually built to withstand 10-20 more degrees than CPUs but it's still degradational to them
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_crit_hyst=70000
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_crit=76000

# system-specific settings
#/sys/class/hwmon/hwmon3/pwm1_enable=2
#/sys/class/hwmon/hwmon3/pwm1=164
#/sys/class/hwmon/hwmon3/pwm2_enable=1
#/sys/class/hwmon/hwmon3/pwm2=90
#/sys/class/hwmon/hwmon3/pwm3_enable=2
#/sys/class/hwmon/hwmon3/pwm3=164

[sysctl]
# disable for lower RAM access latency (may cause severe fragmentation) or enable for efficiency (may hurt realtime tasks)
vm.compact_unevictable_allowed=1
vm.hugepages_treat_as_movable=1
# default of this is based on RAM-size (66 MB with 16 GB for me, for example) and works well enough but we want more
vm.min_free_kbytes=262144
# clustered servers may want to keep it at 0 but 3 could be a safe compromise between latency and efficiencyâ€¦ if it worked BUT in reality it only wastes CPU and brings I/O to a crawl with no benefit
# https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/
vm.zone_reclaim_mode=0
# how aggressive reclaim is, default is 1%
vm.min_unmapped_ratio=5
# keep processes in RAM that is controlled by CPU that is running them
kernel.numa_balancing=1

# better clock:
# http://wiki.linuxaudio.org/wiki/system_configuration
dev.hpet.max-user-freq=4096

# may be already selected as kernel's default
# https://wiki.gentoo.org/wiki/Traffic_shaping#Theory
#net.core.default_qdisc=fq_codel

# it may actually hurt load balancing:
# https://unix.stackexchange.com/questions/277505/why-is-nice-level-ignored-between-different-login-sessions-honoured-if-star
#kernel.sched_autogroup_enabled=0

# Minimal preemption granularity for CPU-bound tasks:
# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
kernel.sched_min_granularity_ns=100000

# The total time the scheduler will consider a migrated process
# "cache hot" and thus less likely to be re-migrated
# (system default is 500000, i.e. 0.5 ms)
kernel.sched_migration_cost_ns=10000000

# inspired by https://forums.gentoo.org/viewtopic-p-8001720.html
kernel.sched_latency_ns=1000000
kernel.sched_wakeup_granularity_ns=1000
# aggressiveness of task migration between CPUs
# https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_MRG/1.0/html/Realtime_Tuning_Guide/sect-Realtime_Tuning_Guide-Realtime_Specific_Tuning-Using_sched_nr_migrate_to_limit_SCHED_OTHER_processes..html
kernel.sched_nr_migrate=2
kernel.sched_cfs_bandwidth_slice_us=1000
# https://www.suse.com/documentation/sles-12/book_sle_tuning/data/sec_tuning_taskscheduler_cfs.html
kernel.sched_time_avg_ms=40
kernel.sched_tunable_scaling=1

# for JACK and better realtime
kernel.sched_rt_period_us=1000000
# this doesn't work with CONFIG_RT_GROUP_SCHED and must be -1 (unlimited) which is dangerous due to system lock-ups
kernel.sched_rt_runtime_us=900000
# granularity of RT. 25 is recommended, 100 is default
kernel.sched_rr_timeslice_ms=16

# some stuff inspired by stock tuned rt profile
# see https://www.kernel.org/doc/Documentation/sysctl/net.txt
kernel.hung_task_timeout_secs=600
vm.stat_interval=10
net.ipv4.tcp_fastopen=3
net.core.dev_weight=128
net.core.dev_weight_rx_bias=6
net.core.dev_weight_tx_bias=2
#net.core.busy_read=50
net.core.busy_poll=100
net.core.netdev_budget=10240
net.core.netdev_budget_usecs=5000
net.core.netdev_tstamp_prequeue=0

# in RAM-caching we prefer extremely large read cache and fairly low write cache

# If a workload mostly uses anonymous memory and it hits this limit, the entire working set is buffered for I/O, and any more write buffering would require swapping, so it's time to throttle writes until I/O can catch up.  Workloads that mostly use file mappings may be able to use even higher values.
# The generator of dirty data starts writeback at this percentage (system default is 20%)
vm.dirty_ratio=10

# Start background writeback (via writeback threads) at this percentage (system default is 10%)
vm.dirty_background_ratio=5

# give it a minute
vm.dirty_expire_centisecs=6000
vm.dirty_writeback_centisecs=1000

# https://unix.stackexchange.com/questions/30286/can-i-configure-my-linux-system-for-more-aggressive-file-system-caching
vm.vfs_cache_pressure=25

# The swappiness parameter controls the tendency of the kernel to move processes out of physical memory and onto the swap disk.
# 0 tells the kernel to avoid swapping processes out of physical memory for as long as possible.
# 100 tells the kernel to aggressively swap processes out of physical memory and move them to swap cache.
# 0-10 is good for systems with normal swap but with usage of zswap (compressed swap in RAM) higher values may be more desirable.
vm.swappiness=90
# higher values (4-5) may be better on systems with CPU power to spare for I/O
vm.page-cluster=5
# aggressiveness of swap in freeing memory, 100 means try to keep 1% of memory free, 1000 is the maximum
vm.watermark_scale_factor=500
