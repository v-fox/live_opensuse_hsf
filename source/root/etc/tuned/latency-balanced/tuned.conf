#
# tuned configuration
#

[main]
summary=Less power-hungry version of latency-performance profile
#include=latency-performance

[variables]
# 'tsc=unstable' or 'tsc=nowatchdog' & 'clocksource=hpet highres=on' may be mandatory for reliable system timer, maybe add 'lapic=notscdeadline x2apic_phys apicpmtimer x86_intel_mid_timer=lapic_and_apbt' for Intel CPUs and 'align_va_addr=on' that is default on AMD CPUs
cmdline_main=sysrq_always_enabled tsc=unstable,nowatchdog clocksource=hpet hpet64 highres=on gbpages noautogroup lapic=notscdeadline x86_intel_mid_timer=lapic_and_apbt
cmdline_rcu=rcupdate.rcu_task_ipi_delay=3333 rcutree.rcu_idle_lazy_gp_delay=4 rcutree.rcu_idle_gp_delay=1
# IOMMU stuff: 'iommu=noagp,nofullflush,memaper=2,merge' and 'intel_iommu=forcedac,sm_on', also use some "RAM cache" for TLB with 'swiotlb=65536'
cmdline_io=io_delay=none swiotlb=65536 iommu=on,noagp,nofullflush,memaper=4,merge amd_iommu=on intel_iommu=on,sm_on,tboot_noforce intel-ishtp.ishtp_use_dma=1 ioatdma.ioat_pending_level=4
# PCI[e] optimizations: 'pci=ecrc=on,assign-busses,check_enable_amd_mmconf,realloc,pcie_bus_perf,pcie_scan_all pcie_ports=auto'
cmdline_pci=pci=ecrc=on,ioapicreroute,assign-busses,check_enable_amd_mmconf,realloc,pcie_bus_perf
# ASPM is known to trigger critical issues with providing little power-saving, if any. Same goes for PCIe PM, so do 'pcie_aspm=off pcie_port_pm=off' but 'pcie_aspm.policy=performance' may be a gentler option
# other ASPM options are: 'default' (developer's default, NOT config's default/unchanged !), 'powersave' and 'powersupersave'
# the idiocy of ASPM handling is in the fact that to actually disable it, you may have to 'force'-enable (!) it in kernel (with policy=performance) to allow it to do anything because geniuses at Intel decided "to match Windows" for "less risk"â€¦.
# BIOS may limit power-states of CPU or kernel may be too "shy" to use them unless directly asked by BIOS, so let's force them with 'processor.ignore_ppc=1 processor.ignore_tpc=1'. that may be detrimental for laptops
# also, see https://github.com/torvalds/linux/commit/25de5718356e264820625600a9edca1df5ff26f8 - kernel was made to be more aggressive in downing frequency which is bad for latency-critical tasks
# https://bugzilla.redhat.com/show_bug.cgi?id=463285#c12 - 2 is default, 1 may give much better powersaving and 3 - better latency guarantees
# hacked BIOS' for locked Intel CPUs may contain microcode that unlocks some higher performance states but that requires disabling OS microcode override with 'dis_ucode_ldr'
# some Intel hacks: "processor.ignore_ppc=1 processor.ignore_tpc=1 processor.latency_factor=3 intel_pstate=percpu_perf_limits"
cmdline_power=processor.ignore_ppc=1 processor.ignore_tpc=1 processor.max_cstate=7 processor.latency_factor=4 skew_tick=1 int_pln_enable
# high nvme.io_queue_depth HW max is supposed to be 65535 but it may increse NVMe I/O latency, it may help to set nvme.max_host_mem_size_mb to be no less than GPU GART (256-1024 MB)
# nvme.sgl_threshold=4096 should enable "Scatter-Gather List" on I/O over 4KB and 2097152 - over 2MB that is x86_64's hugepage size, nvme_core.streams is disabled by default, nvme_core.default_ps_max_latency_us=5500 disables deepest power-saving mode
cmdline_storage=nvme.io_queue_depth=65535 nvme.max_host_mem_size_mb=6144 nvme.sgl_threshold=65537 nvme_core.streams=1 nvme_core.default_ps_max_latency_us=5500 nvme_core.admin_timeout=900 nvme_core.io_timeout=600 nvme_core.shutdown_timeout=30
# MSI-capable NVMe running on old Intel system may result in boot failure "INTR-REMAP:[fault reason 38]" (https://bugs.launchpad.net/ubuntu/+source/linux/+bug/605686), workaround is 'intremap=nosid'
# amdgpu tends to completely shit itself on init on some systems when 64-bit PCIe adressing (AKA "above 4GB decoding") is enabled and "host bridge windows from ACPI" are "used" in PCI[e], so 'pci=nocrs' needs to be added
# amdgpu's "new output framework" shits the bed in pure UEFI video mode by permanently disabling the output port that was used by UEFI, so disable it by 'amdgpu.dc=0'
# although, it can be worked around also by using UEFI CSM compatibility and legacy VBIOS since amdgpu forces full modeset reinit either way
cmdline_workarounds=add_efi_memmap pci=big_root_window,skip_isa_align,nocrs mce=recovery iomem=relaxed acpi_enforce_resources=lax intremap=nosid,no_x2apic_optout
# ignore vulnerabilities and disable mitigations that result in huge loss in performance ? most danger comes from virtual machines with untrusted guests
cmdline_vulnerabilities=tsx=on mitigations=off tsx_async_abort=off pti=off l1tf=off kvm-intel.vmentry_l1d_flush=cond kvm.nx_huge_pages=force mds=off spec_store_bypass_disable=seccomp ssbd=kernel spectre_v2=off spectre_v2_user=seccomp

[bootloader]
cmdline=${cmdline_main} ${cmdline_rcu} ${cmdline_io} ${cmdline_pci} ${cmdline_power} ${cmdline_storage} ${cmdline_workarounds} ${cmdline_vulnerabilities}

[cpu]
#force_latency=1
# this setting is possibly no longer applicable.
# 'schedutil' is considered universal future-proof solution but it's too undercooked yet and might properly work only on AMD.
# 'ondemand' is safe default for all cases.
# on Intel CPUs only "performance" and "powersave" may be available and former completely disables frequency scaling '|' can be used for "if absent then try next"
#governor=schedutil|ondemand
rate_limit_us=4
# https://lore.kernel.org/patchwork/patch/655892/ & https://lkml.org/lkml/2019/3/18/612
energy_perf_bias=performance|balance_performance
min_perf_pct=49
max_perf_pct=100
hwp_dynamic_boost=1
energy_efficiency=0
#no_turbo=0

# this seems broken
#[video]
#radeon_powersave=dpm-balanced,auto

[vm]
# https://www.kernel.org/doc/Documentation/vm/transhuge.txt
# use THP (transparent hugepages of 2M instead of default 4K on x86) to speed up RAM management by avoiding fragmentation & memory controller overload for price of more size overhead.
# 'always' forces them by default, use only with a lot of RAM to spare. Better used in conjunction with kernel boot option of 'transparent_hugepage=always' for early allocation.
# 'madvise' relies on software explicitly requesting them which is the safe option
#transparent_hugepages=always

[script]
# script for autoprobing sensors and configuring network devices
script=script.sh

[sysfs]
# force full speed at the first core of the first CPU so
# non-demanding realtime programs, such as JACK, may be forced on it
#/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor=performance

# intel_pstate hacks from https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt
/sys/kernel/debug/pstate_snb/sample_rate_ms=3
/sys/kernel/debug/pstate_snb/setpoint=80
/sys/kernel/debug/pstate_snb/p_gain_pct=25

# don't run KSM too often
/sys/kernel/mm/ksm/sleep_millisecs=250

# accumpanying options for THP
# tmpfs mounts also should have 'huge=' option with 'always', 'within_size' or 'advise', like in /etc/systemd/system/{dev-shm,tmp}.mount
/sys/kernel/mm/transparent_hugepage/shmem_enabled=advise
# 'defer+madvise' is the most preffered option but it has a small risk of delays breaking realtime, 'always' breaks realtime for sure and creates bad stutters.
# however, tuned seems to shit itself and refuses to use 'defer'.
/sys/kernel/mm/transparent_hugepage/defrag=defer+madvise
# separate periodic defrag call is also breaks realtime and creates bad stutters
#/sys/kernel/mm/transparent_hugepage/khugepaged/defrag=0
# make it often and not long to avoid stutters
# 512/1000/10000 values seem tolerable on weak old CPUs
/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan=336
/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs=1111
/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs=3333
# for x86_64 arch
/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_none=1022
/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_swap=511

# NVMe I/O scheduling optimizations for minimal random r/w latency
# accommodates kernel NVMe module options above
# https://www.kernel.org/doc/Documentation/block/
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems-configuration_tools
# is there any sense in using any I/O scheduler other than NOOP ?
#/sys/block/nvme*n*/queue/scheduler=mq-deadline
# add all NVMe devices as entropy sources ? this is very effective but may worsen I/O access latency
/sys/block/nvme*n*/queue/add_random=1
## NOOP options
# NVMe supposed to support 65535 queue depth but Linux did not allow me to set more than 10238 under NOOP here and 2048 - under mq-deadline
# HOWEVER, that number is requests per application's I/O "turn" and not overall per device queue
/sys/block/nvme*n*/queue/nr_requests=256
# device-specific, maximum value must be equal to max_hw_sectors_kb but smaller one supposedly improves I/O latency
# for me, sometimes it decided to have 2048 or 256
# 2048kb is equal to a hugepage on x86_64 and AMD GPUs, 4kb is default filesystem block size
# queue memory is 2*nr_requests*max_sectors_kb so 2*8192*64KB=1GB
/sys/block/nvme*n*/queue/max_sectors_kb=2048
# disabling it with 0 SHOULD HAVE removed unnecessary latency BUT INSTEAD IT COMPLETELY KILLS THROUGHPUT !!!
# so set it to no less than max_hw_sectors_kb, 256-2048 range might be good, 1024 seem to be a sweet spot
/sys/block/nvme*n*/queue/read_ahead_kb=2048
# https://events.static.linuxfound.org/sites/events/files/slides/lemoal-nvme-polling-vault-2017-final_0.pdf
# -1 is default (high CPU-load);
# 0 is "adaptive hybrid polling" (best latency with moderate CPU-load);
# >0 is fixed-time hybrid polling in ns (device-specific; 2 may be close to 0, 4 - minimal CPU load with latency as high as -1)
/sys/block/nvme*n*/queue/io_poll_delay=0
# actually enable it, if possible
/sys/block/nvme*n*/queue/io_poll=1
# don't use CPU for the work of NVMe controller ?
# 1 means 'simple merges', 2 is to actually disable them
#/sys/block/nvme*n*/queue/nomerges=1
# supposedly decreases CPU load due to caching. 0 to disable, 1 for "CPU group", 2 is strict CPU binding
/sys/block/nvme*n*/queue/rq_affinity=1
# target read-latency, default is 2000 (2ms ?)
/sys/block/nvme*n*/queue/wbt_lat_usec=250
## MQ-DEADLINE options
/sys/block/nvme*n*/queue/iosched/front_merges=0
# must be lower for better latency
/sys/block/nvme*n*/queue/iosched/fifo_batch=4
/sys/block/nvme*n*/queue/iosched/writes_starved=4
/sys/block/nvme*n*/queue/iosched/read_expire=50
/sys/block/nvme*n*/queue/iosched/write_expire=1000

## attempt in controlling PWM fans
# we need to make sure that all this is done after module probing and autoloading
# it would be nice to set it to not spin lower than 2 times per second (120 RPM) BUT if that's not 0 then fan alarm will loose its shit
#/sys/class/hwmon/hwmon?/fan?_min=0
# SHUT UP
/sys/class/hwmon/hwmon?/temp?_beep=0
/sys/class/hwmon/hwmon?/fan?_beep=0
/sys/class/hwmon/hwmon?/fan?_alarm=0

# modern CPUs and their thermal pastes are designed to be no hotter than 70 degrees
# attempt to force "use fan at 32-70 degrees Celsius where 70 is critical, try to use ~80% at initialization and ~100% fan speed at 60 degrees" rule on motherboard
/sys/class/hwmon/hwmon?/temp?_min=22000
/sys/class/hwmon/hwmon?/temp?_max=77000
/sys/class/hwmon/hwmon?/temp?_crit_hyst=87000
/sys/class/hwmon/hwmon?/temp?_crit=90000
# 23437 is a default safe value (anything above hearing frequency of 22 KHz, about 25 KHz), lower ones (like 10190) may work better but they also produce noticeable noise
#/sys/class/hwmon/hwmon?/pwm?_freq=46875
# apparently, 0="full auto" (temperature-based ?), 1="open-loop" (manual, based on value 0-255 in pwm?) and 2="closed-loop" (based on target RPM or some other weird crap)
# on my system with broken it87 module setting '0' forces constant 100%, '1' forces constant hardcoded value (and wrong one at that) and '2' is getting stuck near last used value
# '2' may require fan IDs to be consistens with temperature sensor IDs which is unrealistic (for me they are reversed)
#/sys/class/hwmon/hwmon?/pwm?=145
# this may behave weirdly if applied to all fans at once, so do it only for the CPU one
#/sys/class/hwmon/hwmon?/pwm1_enable=2

# AMD GPU is locked to about 20% for some reason by default, try to force it to spin at 100% close to 65 degrees and set about 60% by default, so it would, at worst, be stuck on a useful speed
# 0 seem to be forcing constant 100% and 1 is fully manual
#/sys/class/drm/card?/device/hwmon/hwmon?/pwm?=128
/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_min=32
/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_max=160
# on '2' amdgpu get's stuck at 13XX and on 0 it's just 100% meaning that this whole thing is broken
#/sys/class/drm/card?/device/hwmon/hwmon?/pwm?_enable=2
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_min=42000
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_max=67000
# GPUs are usually built to withstand 10-20 more degrees than CPUs but it's still degradational to them
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_crit_hyst=77000
#/sys/class/drm/card?/device/hwmon/hwmon?/temp?_crit=80000

# system-specific settings
## AM3
#/sys/class/hwmon/hwmon3/pwm1_enable=1
#/sys/class/hwmon/hwmon3/pwm1=125
#/sys/class/hwmon/hwmon3/temp1_max=80000
#/sys/class/hwmon/hwmon3/pwm2_enable=1
#/sys/class/hwmon/hwmon3/pwm2=90
#/sys/class/hwmon/hwmon3/temp2_max=70000
#/sys/class/hwmon/hwmon3/pwm3_enable=2
#/sys/class/hwmon/hwmon3/pwm3=128
#/sys/class/hwmon/hwmon3/temp3_max=67000
# X79-P3
# pwm_mode 1 is static, 2 and 3 are some thermal and speed "cruise" modes respectively, 4 and 5 are Intel's "Smart Fan" versions 3 and 4
#/sys/class/hwmon/hwmon2/pwm?_auto_point1_pwm=44
#/sys/class/hwmon/hwmon2/pwm?_auto_point1_temp=25000
#/sys/class/hwmon/hwmon2/pwm?_auto_point2_pwm=88
#/sys/class/hwmon/hwmon2/pwm?_auto_point2_temp=45000
#/sys/class/hwmon/hwmon2/pwm?_auto_point3_pwm=128
#/sys/class/hwmon/hwmon2/pwm?_auto_point3_temp=60000
#/sys/class/hwmon/hwmon2/pwm?_auto_point4_pwm=172
#/sys/class/hwmon/hwmon2/pwm?_auto_point4_temp=70000
#/sys/class/hwmon/hwmon2/pwm?_auto_point5_pwm=224
#/sys/class/hwmon/hwmon2/pwm?_auto_point5_temp=80000
#/sys/class/hwmon/hwmon2/pwm?_target_temp=68000
#/sys/class/hwmon/hwmon2/pwm?_floor=10
#/sys/class/hwmon/hwmon2/pwm?_start=15
# force DC over PWM
#/sys/class/hwmon/hwmon2/pwm?_mode=0
# fix wrong temp/fan pairs
#/sys/class/hwmon/hwmon2/pwm?_temp_sel=7
# exceptions
#/sys/class/hwmon/hwmon2/pwm2_temp_sel=1
#/sys/class/hwmon/hwmon2/pwm?_enable=5
#/sys/class/hwmon/hwmon2/pwm?=90
#/sys/class/hwmon/hwmon2/pwm1_enable=1
#/sys/class/hwmon/hwmon2/pwm1=90

[sysctl]
# disable for lower RAM access latency (may cause severe fragmentation) or enable for efficiency (may hurt realtime tasks)
vm.compact_unevictable_allowed=1
#vm.hugepages_treat_as_movable=1
# default of this is based on RAM-size (66 MB with 16 GB for me, for example) and works well enough but we want more
vm.min_free_kbytes=262144
# clustered servers may want to keep it at 0 but 3 could be a safe compromise between latency and efficiencyâ€¦ if it worked BUT in reality it only wastes CPU and brings I/O to a crawl with no benefit
# https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/
vm.zone_reclaim_mode=0
# how aggressive reclaim is, default is 1%
vm.min_unmapped_ratio=5
# enable to keep processes in RAM that is controlled by CPU that is running them
# multi-socket systems must have this enabled
kernel.numa_balancing=0
kernel.numa_balancing_scan_period_min_ms=64
kernel.numa_balancing_scan_period_max_ms=10000
kernel.numa_balancing_scan_delay_ms=256
kernel.numa_balancing_scan_size_mb=16

# better clock:
# http://wiki.linuxaudio.org/wiki/system_configuration
dev.hpet.max-user-freq=4096

# may be already selected as kernel's default
# https://wiki.gentoo.org/wiki/Traffic_shaping#Theory
# sfq is simple and safe, fq_codel and complex and robust, cake is even more complex but lacks some tunable parameters
net.core.default_qdisc=fq_codel

# it may actually hurt load balancing:
# https://unix.stackexchange.com/questions/277505/why-is-nice-level-ignored-between-different-login-sessions-honoured-if-star
kernel.sched_autogroup_enabled=0

# inspired by https://forums.gentoo.org/viewtopic-p-8001720.html and https://probablydance.com/2019/12/30/measuring-mutexes-spinlocks-and-how-bad-the-linux-scheduler-really-is/
kernel.sched_latency_ns=100000
# Minimal preemption granularity for CPU-bound tasks:
# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
kernel.sched_min_granularity_ns=100000
kernel.sched_wakeup_granularity_ns=99
# aggressiveness of task migration between CPUs
# https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_MRG/1.0/html/Realtime_Tuning_Guide/sect-Realtime_Tuning_Guide-Realtime_Specific_Tuning-Using_sched_nr_migrate_to_limit_SCHED_OTHER_processes..html
# RT-optimized is 2, default is 8, 32 seems safe even for RT and for modern high-thread CPUs and many-thread operating systems much more may be viable as long as kernel.sched_rr_timeslice_ms is low enough
kernel.sched_nr_migrate=12
# The total time the scheduler will consider a migrated process
# "cache hot" and thus less likely to be re-migrated
# (system default is 500000, i.e. 0.5 ms)
# supposedly, 4GHz RISC CPU performs an operation every 0.25ns BUT x86 is CISC with on-CPU TLB unable to hold entire table for memory mapping
# therefore, it can't reliably finish even a single operation in 1 cycle of work, let alone, an entire "timeslice" of code
kernel.sched_migration_cost_ns=99000
# https://bugzilla.redhat.com/show_bug.cgi?id=1797629
kernel.timer_migration=0
kernel.sched_cfs_bandwidth_slice_us=100
# https://www.suse.com/documentation/sles-12/book_sle_tuning/data/sec_tuning_taskscheduler_cfs.html
#kernel.sched_time_avg_ms=8
kernel.sched_tunable_scaling=1
kernel.sched_child_runs_first=1

# for JACK and better realtime
kernel.sched_rt_period_us=2000000
# this doesn't work with CONFIG_RT_GROUP_SCHED and must be -1 (unlimited) which is dangerous due to system lock-ups
kernel.sched_rt_runtime_us=1500000
# granularity of RT. 25 is recommended, 100 is default.
# this may influence system responsiveness and allow RT tasks to be scheduled with better granularity than default CFS constrains
kernel.sched_rr_timeslice_ms=1

# some stuff inspired by stock tuned rt profile
# see https://www.kernel.org/doc/Documentation/sysctl/net.txt
# https://github.com/leandromoreira/linux-network-performance-parameters
kernel.hung_task_check_interval_secs=5
kernel.hung_task_timeout_secs=30
vm.stat_interval=10
net.ipv4.tcp_fastopen=3
# default is 64 with 1/1 for rx/tx, 39 seems safe for rx/tx bias 3/2, 472 seems fine
# set weight to [ netdev_budget / weight_rx_bias ] ?
net.core.dev_weight=64
net.core.dev_weight_rx_bias=3
net.core.dev_weight_tx_bias=2
#net.core.busy_read=0
# should be 100 for maximum network performance but anything â‰¥50 is a CPU strain
net.core.busy_poll=11
net.core.netdev_max_backlog=16384
# 700-871 may be optimal but will eat a lot of CPU-time
# <200 may lead to dropped packets even on slow links but 117 seems safe
net.core.netdev_budget=384
# should be â‰¥5000 for maximum network performance, small values may result in dropped frames & packets
# 751 seems safe, 999 may be optimal, 1999 is decent for networking performance
net.core.netdev_budget_usecs=3999
#net.core.netdev_tstamp_prequeue=0
# https://github.com/leandromoreira/linux-network-performance-parameters
# attemp in gaining minimal buffer by default with window for up to 10G
net.core.optmem_max=1048576
net.core.rmem_default=1048576
net.core.rmem_max=134217728
net.core.wmem_default=1048576
net.core.wmem_max=134217728
# tcp/udp_mem is supposedly in 4K pages
# so set it to min / pressure / max as 128MB / 768MB / 1GB
# too low may result in randomly dropped packets
net.ipv4.tcp_mem=32768 196608 262144
net.ipv4.tcp_rmem=16384 1048576 134217728
net.ipv4.tcp_wmem=16384 1048576 134217728
net.ipv4.udp_mem=32768 196608 262144
net.ipv4.udp_rmem_min=16384
net.ipv4.udp_wmem_min=16384
# https://www.kernel.org/doc/Documentation/networking/scaling.txt
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-networking-configuration_tools#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Receive_Packet_Steering_RPS
# net.core.rps_sock_flow_entries should be a number of maximum expected system-wide simultaneous connections
net.core.rps_sock_flow_entries=1024

# BPF
#net.core.bpf_jit_enable=1
#net.core.bpf_jit_harden=1
net.core.bpf_jit_kallsyms=1
net.core.bpf_jit_limit=1073741824

# in RAM-caching we prefer extremely large read cache and fairly low write cache

# If a workload mostly uses anonymous memory and it hits this limit, the entire working set is buffered for I/O, and any more write buffering would require swapping, so it's time to throttle writes until I/O can catch up.  Workloads that mostly use file mappings may be able to use even higher values.
# The generator of dirty data starts writeback at this percentage (system default is 20%)
vm.dirty_ratio=10

# Start background writeback (via writeback threads) at this percentage (system default is 10%)
vm.dirty_background_ratio=5

# give it a minute
vm.dirty_expire_centisecs=6000
vm.dirty_writeback_centisecs=1000

# https://unix.stackexchange.com/questions/30286/can-i-configure-my-linux-system-for-more-aggressive-file-system-caching
vm.vfs_cache_pressure=25

# The swappiness parameter controls the tendency of the kernel to move processes out of physical memory and onto the swap disk.
# 0 tells the kernel to avoid swapping processes out of physical memory for as long as possible.
# 100 tells the kernel to aggressively swap processes out of physical memory and move them to swap cache.
# 0-10 is good for systems with normal swap but with usage of zswap (compressed swap in RAM) higher values may be more desirable.
vm.swappiness=25
# higher values (4-5) may be better on systems with CPU power to spare for I/O BUT they may increase latencies on SSDs
vm.page-cluster=2
# aggressiveness of swap in freeing memory, 100 means try to keep 1% of memory free, 1000 is the maximum
vm.watermark_scale_factor=500
